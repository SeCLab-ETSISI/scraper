from bs4 import BeautifulSoup
import urllib.request

def A単adirURLalTXT(url,tocheck):
  html_page = urllib.request.urlopen(url)
  soup = BeautifulSoup(html_page, "html.parser")
  with open('output.txt', 'a') as f:
    for link in soup.findAll('a'):
      if link.get('href').startswith(tocheck):
        f.write(link.get('href'))
        f.write("\n")


def remove_duplicates(input_file, output_file):
	lines_seen = set()
	with open(output_file, 'w') as out_file:
		with open(input_file, 'r') as in_file:
			for line in in_file:
				if line not in lines_seen:
					out_file.write(line)
					lines_seen.add(line)

def remove_discus(input_file, output_file):
  with open(output_file, 'w') as out_file:
    with open(input_file, 'r') as in_file:
      for line in in_file:
        if "#disqus_thread" not in line:
          out_file.write(line)




tochck = "https://www.malwarebytes.com/blog/threat-intelligence/"
url = "https://www.malwarebytes.com/blog/category/threat-intelligence"
A単adirURLalTXT(url,tochck)

i = 2;
nuevaURL = "https://www.malwarebytes.com/blog/category/threat-intelligence/page/2"
A単adirURLalTXT(nuevaURL,tochck)
request = requests.get(nuevaURL)

while(request.status_code == 200):
  A単adirURLalTXT(nuevaURL,tochck)
  i = i +1
  l = "https://www.malwarebytes.com/blog/category/threat-intelligence/page/"
  nuevaURL = f"{l}{i}"
  request = requests.get(nuevaURL)

remove_duplicates("output.txt", "outputR.txt")
remove_discus("outputR.txt","malwareBytes.txt")

#queda borrar output.txt y outputR.txt 
